{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f40c414-1ad4-4d5e-8581-b4033294a6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import time, json, datetime \n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89f805af-3997-4a7b-97f9-fbcd6b37f717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train： (700146, 9)\n",
      "test: (300063, 9)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_pickle('data/ml-1m.pkl')\n",
    "\n",
    "feat_col = ['userId', 'gender', 'age', 'occupation', 'movieId', 'year']\n",
    "label_col = 'rating'\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.3, random_state=20220316)\n",
    "print('train：', train.shape)\n",
    "print('test:', test.shape)\n",
    "\n",
    "user_id_col = 'userId'\n",
    "user_attr_col = ['gender', 'age', 'occupation']\n",
    "item_id_col = 'movieId'\n",
    "item_attr_col = ['year']\n",
    "label_col = 'rating'\n",
    "\n",
    "train_dataset = TensorDataset(torch.LongTensor(train[user_id_col].values),\n",
    "                              torch.LongTensor(train[user_attr_col].values),\n",
    "                              torch.LongTensor(train[item_id_col].values),\n",
    "                              torch.LongTensor(train[item_attr_col].values),\n",
    "                              torch.FloatTensor(train[label_col].values),)\n",
    "test_dataset = TensorDataset(torch.LongTensor(test[user_id_col].values),\n",
    "                              torch.LongTensor(test[user_attr_col].values),\n",
    "                              torch.LongTensor(test[item_id_col].values),\n",
    "                              torch.LongTensor(test[item_attr_col].values),\n",
    "                              torch.FloatTensor(test[label_col].values),)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2048, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4096, shuffle=True)\n",
    "\n",
    "def print_info(info):\n",
    "    t0 = datetime.datetime.now().strftime('%H:%M:%S')\n",
    "    info = '{} : {}'.format(t0, info)\n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c328feb-ac6d-4680-8638-ebb01b5e3109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1104</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 176, 1050, 2409, 2410, 0, 0, 0]</td>\n",
       "      <td>[10]</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>1104</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 176, 1050, 2409, 2410, 0, 0, 0]</td>\n",
       "      <td>[10]</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1104</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 176, 1050, 2409, 2410, 0, 0, 0]</td>\n",
       "      <td>[10]</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1104</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 176, 1050, 2409, 2410, 0, 0, 0]</td>\n",
       "      <td>[10]</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1104</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 176, 1050, 2409, 2410, 0, 0, 0]</td>\n",
       "      <td>[10]</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  gender  age  occupation  movieId  rating  \\\n",
       "0       0       0    0          10     1104       1   \n",
       "1       1       1    6          16     1104       1   \n",
       "2      11       1    2          12     1104       1   \n",
       "3      14       1    2           7     1104       1   \n",
       "4      16       1    5           1     1104       1   \n",
       "\n",
       "                                 title genres  year  \n",
       "0  [1, 176, 1050, 2409, 2410, 0, 0, 0]   [10]    76  \n",
       "1  [1, 176, 1050, 2409, 2410, 0, 0, 0]   [10]    76  \n",
       "2  [1, 176, 1050, 2409, 2410, 0, 0, 0]   [10]    76  \n",
       "3  [1, 176, 1050, 2409, 2410, 0, 0, 0]   [10]    76  \n",
       "4  [1, 176, 1050, 2409, 2410, 0, 0, 0]   [10]    76  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00b1da8d-c82f-4ddc-856d-d93fcd43a463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KLD_Gaussian(mu_q, sigma_q, mu_p, sigma_p):\n",
    "    return torch.log(sigma_p / sigma_q) + (sigma_q**2 + (mu_q - mu_p)**2) / (2 * sigma_p**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57687aae-6b50-43cc-8daa-d7638bb2808a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VELBase(nn.Module):\n",
    "    def __init__(self, feature_nuniques_list, emb_size=8):\n",
    "        super().__init__()\n",
    "        '''\n",
    "        feature_nuniques_list: a list of  features' vocabulary size\n",
    "        for instance, feature_nuniques_list = [2000, [2, 8, 10], 1000, [7, 9]]\n",
    "        '''\n",
    "        user_id_size, user_attr_size, item_id_size, item_attr_size = feature_nuniques_list\n",
    "        self.user_id_emb = nn.Embedding(user_id_size, emb_size) \n",
    "        self.user_id_dnn = nn.Sequential(nn.Linear(emb_size, 256), nn.ReLU(),\n",
    "                                      nn.Linear(256, emb_size*2), nn.Sigmoid())\n",
    "        \n",
    "        self.user_attr_emb = nn.ModuleList([nn.Embedding(voc_size, emb_size) \n",
    "                                            for voc_size in user_attr_size])\n",
    "        self.user_attr_dim = emb_size * len(user_attr_size)\n",
    "        self.user_attr_dnn = nn.Sequential(nn.Linear(self.user_attr_dim, 256), nn.ReLU(),\n",
    "                                           nn.Linear(256, emb_size*2), nn.Sigmoid())\n",
    "        \n",
    "        self.item_id_emb = nn.Embedding(item_id_size, emb_size)\n",
    "        self.item_id_dnn = nn.Sequential(nn.Linear(emb_size, 256), nn.ReLU(),\n",
    "                                      nn.Linear(256, emb_size*2), nn.Sigmoid())\n",
    "        \n",
    "        self.item_attr_emb = nn.ModuleList([nn.Embedding(voc_size, emb_size) \n",
    "                                          for voc_size in item_attr_size])\n",
    "        self.item_attr_dim = emb_size * len(item_attr_size)\n",
    "        self.item_attr_dnn = nn.Sequential(nn.Linear(self.item_attr_dim, 256), nn.ReLU(),\n",
    "                                           nn.Linear(256, emb_size*2), nn.Sigmoid())\n",
    "        self.feature_dims = emb_size * 2 + self.user_attr_dim + self.item_attr_dim\n",
    "        \n",
    "    def forward(self, user_id, user_attr, item_id, item_attr):\n",
    "        # variational embedding for user id\n",
    "        user_id_emb_res = self.user_id_emb(user_id)\n",
    "        user_id_dnn_res = self.user_id_dnn(user_id_emb_res)\n",
    "        user_id_mu, user_id_sigma = user_id_dnn_res.chunk(2, dim=1)\n",
    "        user_id_sigma = torch.abs(user_id_sigma)\n",
    "        # Reparameterize Trick\n",
    "        user_id_vemb = user_id_mu + user_id_sigma * torch.randn_like(user_id_sigma)\n",
    "        # embedding for user attribute\n",
    "        user_attr_emb_res = [emb(user_attr[:, i]) for i, emb in enumerate(self.user_attr_emb)]\n",
    "        user_attr_emb_concat = torch.cat(user_attr_emb_res, dim=1)\n",
    "        user_attr_dnn_res = self.user_attr_dnn(user_attr_emb_concat)\n",
    "        user_attr_mu, user_attr_sigma = user_attr_dnn_res.chunk(2, dim=1)\n",
    "        user_attr_sigma = torch.abs(user_attr_sigma)\n",
    "        # variational embedding for item id\n",
    "        item_id_emb_res = self.item_id_emb(item_id)\n",
    "        item_id_dnn_res = self.item_id_dnn(item_id_emb_res)\n",
    "        item_id_mu, item_id_sigma = item_id_dnn_res.chunk(2, dim=1)\n",
    "        item_id_sigma = torch.abs(item_id_sigma)\n",
    "        # Reparameterize Trick\n",
    "        item_id_vemb = item_id_mu + item_id_sigma * torch.randn_like(item_id_sigma)\n",
    "        # embedding for item attribute\n",
    "        item_attr_emb_res = [emb(item_attr[:, i]) for i, emb in enumerate(self.item_attr_emb)]\n",
    "        item_attr_emb_concat = torch.cat(item_attr_emb_res, dim=1)\n",
    "        item_attr_dnn_res = self.item_attr_dnn(item_attr_emb_concat)\n",
    "        item_attr_mu, item_attr_sigma = item_attr_dnn_res.chunk(2, dim=1)\n",
    "        item_attr_sigma = torch.abs(item_attr_sigma)\n",
    "        # concat all embeddings\n",
    "        all_embs = torch.cat([user_id_vemb, item_id_vemb, user_attr_emb_concat, item_attr_emb_concat], dim=1)\n",
    "        # users' KL-divergence\n",
    "        user_kld = KLD_Gaussian(user_id_mu, user_id_sigma, user_attr_mu, user_attr_sigma)\n",
    "        # items' KL-divergence\n",
    "        item_kld = KLD_Gaussian(item_id_mu, item_id_sigma, item_attr_mu, item_attr_sigma)\n",
    "        # user's prior KL-divergence\n",
    "        user_prior_kld = KLD_Gaussian(user_attr_mu, user_attr_sigma, 0, 1)\n",
    "        # items's prior KL-divergence\n",
    "        item_prior_kld = KLD_Gaussian(item_attr_mu, item_attr_sigma, 0, 1)\n",
    "        kld = user_kld + item_kld + user_prior_kld + item_prior_kld\n",
    "        kld = torch.mean(kld)\n",
    "        return all_embs, kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "406cafd7-a0cf-4623-b7ec-b56215d22d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VELDeepFM(nn.Module):\n",
    "    def __init__(self, feature_nuniques_list, alpha=1, emb_size=8, \n",
    "                 hid_dims=[256, 128], num_classes=1, dropout=[0.2, 0.2]):\n",
    "        super().__init__()\n",
    "        self.encoder = VELBase(feature_nuniques_list)\n",
    "        self.all_dims = [self.encoder.feature_dims] + hid_dims\n",
    "        self.dnn_linear_list = nn.ModuleList()\n",
    "        for i in range(1, len(self.all_dims)):\n",
    "            self.dnn_linear_list.append(nn.Sequential(\n",
    "                nn.Linear(self.all_dims[i-1], self.all_dims[i]),\n",
    "                nn.BatchNorm1d(self.all_dims[i]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout[i-1])\n",
    "            ))\n",
    "        self.dnn_linear_list.append(nn.Linear(hid_dims[-1], num_classes))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, user_id, user_attr, item_id, item_attr):\n",
    "        all_embs, kld = self.encoder(user_id, user_attr, item_id, item_attr)\n",
    "        '''FM module'''\n",
    "        fm_1st_part = torch.sum(all_embs, 1, keepdim=True)\n",
    "        sum_square_emb = torch.sum(all_embs * all_embs, 1, keepdim=True)\n",
    "        sum_emb = torch.sum(all_embs, 1, keepdim=True)\n",
    "        square_sum_emb = sum_emb * sum_emb\n",
    "        fm_2nd_part = (sum_square_emb - square_sum_emb) * 0.5\n",
    "        '''DNN module'''\n",
    "        dnn_out = all_embs\n",
    "        for linear in self.dnn_linear_list:\n",
    "            dnn_out = linear(dnn_out)\n",
    "        out = dnn_out + fm_1st_part + fm_2nd_part\n",
    "        out = self.sigmoid(out)\n",
    "        return out, kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49fcd7f2-36c6-411b-be33-0c38ad4640c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Total': 154953, 'Trainable': 154953}\n"
     ]
    }
   ],
   "source": [
    "feature_nuniques_list = [data[user_id_col].nunique(),\n",
    "                          [data[f].nunique() for f in user_attr_col],\n",
    "                          data[item_id_col].nunique(),\n",
    "                          [data[f].nunique() for f in item_attr_col]]\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = VELDeepFM(feature_nuniques_list, 1)\n",
    "model.to(device)\n",
    "\n",
    "loss = nn.BCELoss() # Binary Cross Entropy Loss\n",
    "loss = loss.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005, weight_decay=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.8)\n",
    "\n",
    "total = sum([param.nelement() for param in model.parameters()])\n",
    "trainable = sum([param.nelement() for param in model.parameters() if param.requires_grad])\n",
    "print({'Total': total, 'Trainable': trainable})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5e0be4a-1ec8-485e-97f9-e3b5e8c72372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(model, train_loader, test_loader, epochs, device):\n",
    "    best_auc = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        '''training process'''\n",
    "        model.train()\n",
    "        print(\"Current lr : {}\".format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "        train_loss_sum = 0.0\n",
    "        start_time = time.time()\n",
    "        for idx, x in enumerate(train_loader):\n",
    "            user_id, user_attr, item_id, item_attr, label = x[0], x[1], x[2], x[3], x[4]\n",
    "            user_id, user_attr = user_id.to(device), user_attr.to(device)\n",
    "            item_id, item_attr = item_id.to(device), item_attr.to(device)\n",
    "            label = label.float().to(device)\n",
    "            pred, kld = model(user_id, user_attr, item_id, item_attr)\n",
    "            pred = pred.view(-1)\n",
    "            l = loss(pred, label) + kld\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss_sum += l.cpu().item()\n",
    "            if (idx+1) % 50 == 0 or (idx + 1) == len(train_loader):\n",
    "                info = 'Epoch: {:04d} | Step: {:04d} / {} | Loss: {:.4f} | Time: {:.4f}'.format(\n",
    "                          epoch+1, idx+1, len(train_loader), train_loss_sum/(idx+1), time.time() - start_time)\n",
    "                print_info(info)     \n",
    "            \n",
    "        scheduler.step()\n",
    "        \n",
    "        '''inference process'''\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_labels, test_preds = [], []\n",
    "            for idx, x in tqdm(enumerate(test_loader)):\n",
    "                user_id, user_attr, item_id, item_attr, label = x[0], x[1], x[2], x[3], x[4]\n",
    "                user_id, user_attr = user_id.to(device), user_attr.to(device)\n",
    "                item_id, item_attr = item_id.to(device), item_attr.to(device)\n",
    "                label = label.float().to(device)\n",
    "                pred, kld = model(user_id, user_attr, item_id, item_attr)\n",
    "                pred = pred.view(-1).data.cpu().numpy().tolist()\n",
    "                test_preds.extend(pred)\n",
    "                test_labels.extend(label.cpu().numpy().tolist())\n",
    "        cur_auc = roc_auc_score(test_labels, test_preds)\n",
    "        if cur_auc > best_auc:\n",
    "            best_auc = cur_auc\n",
    "            torch.save(model.state_dict(), 'deep_fm_ml_1m.pth')   \n",
    "        info = 'Current AUC: {:.6f}, Best AUC: {:.6f}\\n'.format(cur_auc, best_auc)\n",
    "        print_info(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac3af584-aff0-4baa-a3e0-d38b0e0414ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current lr : 0.005\n",
      "10:54:20 : Epoch: 0001 | Step: 0050 / 342 | Loss: 12.9680 | Time: 3.0893\n",
      "10:54:23 : Epoch: 0001 | Step: 0100 / 342 | Loss: 8.3305 | Time: 6.0299\n",
      "10:54:26 : Epoch: 0001 | Step: 0150 / 342 | Loss: 6.6617 | Time: 8.9614\n",
      "10:54:29 : Epoch: 0001 | Step: 0200 / 342 | Loss: 5.8014 | Time: 12.0127\n",
      "10:54:32 : Epoch: 0001 | Step: 0250 / 342 | Loss: 5.2725 | Time: 14.9076\n",
      "10:54:35 : Epoch: 0001 | Step: 0300 / 342 | Loss: 4.9123 | Time: 17.8472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  8.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:54:37 : Epoch: 0001 | Step: 0342 / 342 | Loss: 4.6858 | Time: 20.2866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "74it [00:07, 10.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:54:45 : Current AUC: 0.534515, Best AUC: 0.534515\n",
      "\n",
      "Current lr : 0.004\n",
      "10:54:48 : Epoch: 0002 | Step: 0050 / 342 | Loss: 3.0327 | Time: 3.1360\n",
      "10:54:51 : Epoch: 0002 | Step: 0100 / 342 | Loss: 3.0304 | Time: 6.1164\n",
      "10:54:54 : Epoch: 0002 | Step: 0150 / 342 | Loss: 3.0269 | Time: 9.3441\n",
      "10:54:57 : Epoch: 0002 | Step: 0200 / 342 | Loss: 3.0205 | Time: 12.3148\n",
      "10:55:00 : Epoch: 0002 | Step: 0250 / 342 | Loss: 3.0138 | Time: 15.4050\n",
      "10:55:03 : Epoch: 0002 | Step: 0300 / 342 | Loss: 3.0076 | Time: 18.4608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  8.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:55:06 : Epoch: 0002 | Step: 0342 / 342 | Loss: 3.0032 | Time: 21.1346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "74it [00:07, 10.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:55:13 : Current AUC: 0.543712, Best AUC: 0.543712\n",
      "\n",
      "Current lr : 0.0032\n",
      "10:55:16 : Epoch: 0003 | Step: 0050 / 342 | Loss: 2.9714 | Time: 3.0981\n",
      "10:55:20 : Epoch: 0003 | Step: 0100 / 342 | Loss: 2.9617 | Time: 6.2987\n",
      "10:55:23 : Epoch: 0003 | Step: 0150 / 342 | Loss: 2.9574 | Time: 9.3196\n",
      "10:55:26 : Epoch: 0003 | Step: 0200 / 342 | Loss: 2.9575 | Time: 12.4030\n",
      "10:55:29 : Epoch: 0003 | Step: 0250 / 342 | Loss: 2.9521 | Time: 15.3589\n",
      "10:55:32 : Epoch: 0003 | Step: 0300 / 342 | Loss: 2.9491 | Time: 18.4415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:55:34 : Epoch: 0003 | Step: 0342 / 342 | Loss: 2.9479 | Time: 20.9938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "74it [00:07, 10.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:55:42 : Current AUC: 0.560389, Best AUC: 0.560389\n",
      "\n",
      "Current lr : 0.00256\n",
      "10:55:45 : Epoch: 0004 | Step: 0050 / 342 | Loss: 2.9162 | Time: 3.1401\n",
      "10:55:48 : Epoch: 0004 | Step: 0100 / 342 | Loss: 2.9142 | Time: 6.2829\n",
      "10:55:51 : Epoch: 0004 | Step: 0150 / 342 | Loss: 2.9148 | Time: 9.1996\n",
      "10:55:54 : Epoch: 0004 | Step: 0200 / 342 | Loss: 2.9147 | Time: 12.3135\n",
      "10:55:57 : Epoch: 0004 | Step: 0250 / 342 | Loss: 2.9131 | Time: 15.3716\n",
      "10:56:00 : Epoch: 0004 | Step: 0300 / 342 | Loss: 2.9130 | Time: 18.5616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  9.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:56:03 : Epoch: 0004 | Step: 0342 / 342 | Loss: 2.9115 | Time: 21.0819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "74it [00:07, 10.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:56:10 : Current AUC: 0.572905, Best AUC: 0.572905\n",
      "\n",
      "Current lr : 0.0020480000000000003\n",
      "10:56:13 : Epoch: 0005 | Step: 0050 / 342 | Loss: 2.8995 | Time: 3.1446\n",
      "10:56:16 : Epoch: 0005 | Step: 0100 / 342 | Loss: 2.8961 | Time: 6.1659\n",
      "10:56:19 : Epoch: 0005 | Step: 0150 / 342 | Loss: 2.8960 | Time: 9.2615\n",
      "10:56:22 : Epoch: 0005 | Step: 0200 / 342 | Loss: 2.8946 | Time: 12.3010\n",
      "10:56:26 : Epoch: 0005 | Step: 0250 / 342 | Loss: 2.8937 | Time: 15.4260\n",
      "10:56:29 : Epoch: 0005 | Step: 0300 / 342 | Loss: 2.8924 | Time: 18.5004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  8.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:56:31 : Epoch: 0005 | Step: 0342 / 342 | Loss: 2.8916 | Time: 21.2613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "74it [00:06, 11.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:56:38 : Current AUC: 0.577995, Best AUC: 0.577995\n",
      "\n",
      "Current lr : 0.0016384000000000004\n",
      "10:56:41 : Epoch: 0006 | Step: 0050 / 342 | Loss: 2.8833 | Time: 3.1265\n",
      "10:56:45 : Epoch: 0006 | Step: 0100 / 342 | Loss: 2.8823 | Time: 6.2106\n",
      "10:56:47 : Epoch: 0006 | Step: 0150 / 342 | Loss: 2.8836 | Time: 9.1514\n",
      "10:56:50 : Epoch: 0006 | Step: 0200 / 342 | Loss: 2.8822 | Time: 12.1637\n",
      "10:56:53 : Epoch: 0006 | Step: 0250 / 342 | Loss: 2.8809 | Time: 14.8471\n",
      "10:56:56 : Epoch: 0006 | Step: 0300 / 342 | Loss: 2.8791 | Time: 17.5876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:56:58 : Epoch: 0006 | Step: 0342 / 342 | Loss: 2.8785 | Time: 19.9935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "74it [00:06, 10.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:57:05 : Current AUC: 0.578975, Best AUC: 0.578975\n",
      "\n",
      "Current lr : 0.0013107200000000005\n",
      "10:57:08 : Epoch: 0007 | Step: 0050 / 342 | Loss: 2.8779 | Time: 3.0742\n",
      "10:57:11 : Epoch: 0007 | Step: 0100 / 342 | Loss: 2.8796 | Time: 5.8666\n",
      "10:57:14 : Epoch: 0007 | Step: 0150 / 342 | Loss: 2.8758 | Time: 8.5617\n",
      "10:57:17 : Epoch: 0007 | Step: 0200 / 342 | Loss: 2.8721 | Time: 11.2777\n",
      "10:57:19 : Epoch: 0007 | Step: 0250 / 342 | Loss: 2.8718 | Time: 13.9679\n",
      "10:57:22 : Epoch: 0007 | Step: 0300 / 342 | Loss: 2.8707 | Time: 16.6959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  7.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:57:24 : Epoch: 0007 | Step: 0342 / 342 | Loss: 2.8699 | Time: 18.9246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "74it [00:07,  9.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:57:32 : Current AUC: 0.585616, Best AUC: 0.585616\n",
      "\n",
      "Current lr : 0.0010485760000000005\n",
      "10:57:35 : Epoch: 0008 | Step: 0050 / 342 | Loss: 2.8604 | Time: 2.8899\n",
      "10:57:38 : Epoch: 0008 | Step: 0100 / 342 | Loss: 2.8596 | Time: 5.6722\n",
      "10:57:41 : Epoch: 0008 | Step: 0150 / 342 | Loss: 2.8582 | Time: 8.5327\n",
      "10:57:44 : Epoch: 0008 | Step: 0200 / 342 | Loss: 2.8574 | Time: 11.3319\n",
      "10:57:46 : Epoch: 0008 | Step: 0250 / 342 | Loss: 2.8561 | Time: 14.0716\n",
      "10:57:49 : Epoch: 0008 | Step: 0300 / 342 | Loss: 2.8561 | Time: 16.9652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  8.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:57:52 : Epoch: 0008 | Step: 0342 / 342 | Loss: 2.8560 | Time: 19.4958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "74it [00:06, 10.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:57:59 : Current AUC: 0.587553, Best AUC: 0.587553\n",
      "\n",
      "Current lr : 0.0008388608000000005\n",
      "10:58:02 : Epoch: 0009 | Step: 0050 / 342 | Loss: 2.8523 | Time: 3.1278\n",
      "10:58:05 : Epoch: 0009 | Step: 0100 / 342 | Loss: 2.8536 | Time: 5.9156\n",
      "10:58:08 : Epoch: 0009 | Step: 0150 / 342 | Loss: 2.8524 | Time: 8.6423\n",
      "10:58:11 : Epoch: 0009 | Step: 0200 / 342 | Loss: 2.8520 | Time: 11.7157\n",
      "10:58:13 : Epoch: 0009 | Step: 0250 / 342 | Loss: 2.8517 | Time: 14.4248\n",
      "10:58:16 : Epoch: 0009 | Step: 0300 / 342 | Loss: 2.8507 | Time: 17.1962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:58:18 : Epoch: 0009 | Step: 0342 / 342 | Loss: 2.8499 | Time: 19.4889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "74it [00:06, 10.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:58:26 : Current AUC: 0.590770, Best AUC: 0.590770\n",
      "\n",
      "Current lr : 0.0006710886400000004\n",
      "10:58:29 : Epoch: 0010 | Step: 0050 / 342 | Loss: 2.8510 | Time: 3.0293\n",
      "10:58:31 : Epoch: 0010 | Step: 0100 / 342 | Loss: 2.8475 | Time: 5.9292\n",
      "10:58:35 : Epoch: 0010 | Step: 0150 / 342 | Loss: 2.8485 | Time: 9.0585\n",
      "10:58:37 : Epoch: 0010 | Step: 0200 / 342 | Loss: 2.8473 | Time: 11.8110\n",
      "10:58:40 : Epoch: 0010 | Step: 0250 / 342 | Loss: 2.8466 | Time: 14.5351\n",
      "10:58:43 : Epoch: 0010 | Step: 0300 / 342 | Loss: 2.8458 | Time: 17.8238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  9.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:58:46 : Epoch: 0010 | Step: 0342 / 342 | Loss: 2.8449 | Time: 20.2277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "74it [00:06, 11.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:58:52 : Current AUC: 0.590481, Best AUC: 0.590770\n",
      "\n",
      "Current lr : 0.0005368709120000003\n",
      "10:58:55 : Epoch: 0011 | Step: 0050 / 342 | Loss: 2.8358 | Time: 2.8568\n",
      "10:58:58 : Epoch: 0011 | Step: 0100 / 342 | Loss: 2.8399 | Time: 5.5290\n",
      "10:59:01 : Epoch: 0011 | Step: 0150 / 342 | Loss: 2.8401 | Time: 8.3322\n",
      "10:59:04 : Epoch: 0011 | Step: 0200 / 342 | Loss: 2.8409 | Time: 11.0753\n",
      "10:59:07 : Epoch: 0011 | Step: 0250 / 342 | Loss: 2.8408 | Time: 14.1087\n",
      "10:59:09 : Epoch: 0011 | Step: 0300 / 342 | Loss: 2.8406 | Time: 16.9053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  8.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:59:12 : Epoch: 0011 | Step: 0342 / 342 | Loss: 2.8406 | Time: 19.4058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "74it [00:06, 11.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:59:19 : Current AUC: 0.595966, Best AUC: 0.595966\n",
      "\n",
      "Current lr : 0.0004294967296000003\n",
      "10:59:22 : Epoch: 0012 | Step: 0050 / 342 | Loss: 2.8407 | Time: 3.0310\n",
      "10:59:25 : Epoch: 0012 | Step: 0100 / 342 | Loss: 2.8384 | Time: 5.8364\n",
      "10:59:27 : Epoch: 0012 | Step: 0150 / 342 | Loss: 2.8372 | Time: 8.5370\n",
      "10:59:30 : Epoch: 0012 | Step: 0200 / 342 | Loss: 2.8372 | Time: 11.3479\n",
      "10:59:33 : Epoch: 0012 | Step: 0250 / 342 | Loss: 2.8374 | Time: 14.5130\n",
      "10:59:36 : Epoch: 0012 | Step: 0300 / 342 | Loss: 2.8380 | Time: 17.6817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  6.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:59:39 : Epoch: 0012 | Step: 0342 / 342 | Loss: 2.8380 | Time: 20.1785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "74it [00:06, 11.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:59:46 : Current AUC: 0.594664, Best AUC: 0.595966\n",
      "\n",
      "Current lr : 0.00034359738368000027\n",
      "10:59:49 : Epoch: 0013 | Step: 0050 / 342 | Loss: 2.8335 | Time: 2.8714\n",
      "10:59:52 : Epoch: 0013 | Step: 0100 / 342 | Loss: 2.8349 | Time: 6.2117\n",
      "10:59:55 : Epoch: 0013 | Step: 0150 / 342 | Loss: 2.8337 | Time: 9.5067\n",
      "10:59:58 : Epoch: 0013 | Step: 0200 / 342 | Loss: 2.8322 | Time: 12.2994\n",
      "11:00:01 : Epoch: 0013 | Step: 0250 / 342 | Loss: 2.8311 | Time: 15.0660\n",
      "11:00:04 : Epoch: 0013 | Step: 0300 / 342 | Loss: 2.8315 | Time: 17.8610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:00:06 : Epoch: 0013 | Step: 0342 / 342 | Loss: 2.8314 | Time: 20.2683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "74it [00:06, 10.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:00:13 : Current AUC: 0.595456, Best AUC: 0.595966\n",
      "\n",
      "Current lr : 0.00027487790694400024\n",
      "11:00:16 : Epoch: 0014 | Step: 0050 / 342 | Loss: 2.8341 | Time: 2.8625\n",
      "11:00:19 : Epoch: 0014 | Step: 0100 / 342 | Loss: 2.8329 | Time: 5.7668\n",
      "11:00:22 : Epoch: 0014 | Step: 0150 / 342 | Loss: 2.8336 | Time: 8.5747\n",
      "11:00:24 : Epoch: 0014 | Step: 0200 / 342 | Loss: 2.8327 | Time: 11.2732\n",
      "11:00:27 : Epoch: 0014 | Step: 0250 / 342 | Loss: 2.8319 | Time: 14.1037\n",
      "11:00:30 : Epoch: 0014 | Step: 0300 / 342 | Loss: 2.8322 | Time: 17.0805\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-bdac2257fc77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_and_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-ee92ed560e19>\u001b[0m in \u001b[0;36mtrain_and_test\u001b[0;34m(model, train_loader, test_loader, epochs, device)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtrain_loss_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0muser_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_attr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_attr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0muser_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_attr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_attr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep-ctr/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep-ctr/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep-ctr/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/deep-ctr/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep-ctr/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep-ctr/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_and_test(model, train_loader, test_loader, 30, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3745973f-4e34-4d4f-8d18-64b669b9e877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.tensor([[1, 2, 3], [4, 9, 7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca7ab77d-c412-4b53-89b0-a680f64313b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 9, 7]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "160cc5d0-b66d-488c-8cb4-2f6cfc93ddc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6],\n",
       "        [20]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(x, 1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22285915-be54-4177-94c8-f2aed6c92bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
